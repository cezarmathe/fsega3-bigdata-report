{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis on a TMDB datased of 15000 movies\n",
    "\n",
    "What is the relationship between a movie's popularity and its average vote? Do more popular movies receive higher average votes?\n",
    "\n",
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# DATA_CSV_FILE = 'data/input.csv'\n",
    "DATA_CSV_FILE = 'data/original.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_CSV_FILE, lineterminator='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "**1. Data cleaning**\n",
    "\n",
    "- remove non-English movies\n",
    "- remove movies with less than 100 votes\n",
    "- remove unused columns\n",
    "- remove duplicates \n",
    "- remove empty values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-English movies.\n",
    "df = df[df['original_language'] == 'en']\n",
    "# Remove movies with less than 100 votes.\n",
    "df = df[df['vote_count'] >= 100]\n",
    "# df = df[df['popularity'] <= 100]\n",
    "\n",
    "# Remove unused columns.\n",
    "df = df.drop(\n",
    "  [\n",
    "    'Unnamed: 0',\n",
    "    'adult',\n",
    "    'backdrop_path',\n",
    "    'cast',\n",
    "    'crew',\n",
    "    'genres',\n",
    "    'keywords',\n",
    "    'original_language',\n",
    "    'poster_path',\n",
    "    'release_date',\n",
    "    'video',\n",
    "    'vote_count',\n",
    "  ],\n",
    "  axis=1,\n",
    ")\n",
    "\n",
    "# Remove rows with null values.\n",
    "df = df.dropna()\n",
    "# Fill null values with empty string.\n",
    "df = df.fillna('')\n",
    "# Remove duplicate rows.\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# # Convert release_date to datetime.\n",
    "# df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Data cleaning**\n",
    "\n",
    "No specific transformation needed - `vote_average` and `popularity` are \n",
    "numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and now that you don't have to be perfect, you can be good\n",
      "but you're already perfect\n"
     ]
    }
   ],
   "source": [
    "print(\"and now that you don't have to be perfect, you can be good\")\n",
    "print(\"but you're already perfect\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Data standardization**\n",
    "\n",
    "We need to standardize `vote_average` and `popularity`.\n",
    "\n",
    "Let's find out their `min`, `max`, `mean`, `median` and `stddev`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- vote_average -----\n",
      "count    3252.000000\n",
      "mean        6.824908\n",
      "std         0.661431\n",
      "min         5.500000\n",
      "25%         6.300000\n",
      "50%         6.800000\n",
      "75%         7.300000\n",
      "max         8.700000\n",
      "Name: vote_average, dtype: float64\n",
      "----- popularity -----\n",
      "count    3252.000000\n",
      "mean       40.140065\n",
      "std       102.874918\n",
      "min         0.617000\n",
      "25%        17.177250\n",
      "50%        24.440500\n",
      "75%        39.750750\n",
      "max      4810.649000\n",
      "Name: popularity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('----- vote_average -----')\n",
    "print(df['vote_average'].describe())\n",
    "\n",
    "print('----- popularity -----')\n",
    "print(df['popularity'].describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vote_average` seems to fit in it's advertised range of __0__ - __10__, whereas\n",
    "`popularity` seems to have quite some outliers. We will do Min-Max Normalization\n",
    "for the former and Z-Score Standardization for the latter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vote_average_normalized'] = (df['vote_average'] - df['vote_average'].min()) / (df['vote_average'].max() - df['vote_average'].min())\n",
    "df['popularity_standardized'] = (df['popularity'] - df['popularity'].mean()) / df['popularity'].std()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "**1. Descriptive analysis**\n",
    "\n",
    "Let's re-run the descriptions on processed columns.\n",
    "\n",
    "I've run the two processes and here are the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- vote_average_normalized -----\n",
      "count    3252.000000\n",
      "mean        0.414034\n",
      "std         0.206697\n",
      "min         0.000000\n",
      "25%         0.250000\n",
      "50%         0.406250\n",
      "75%         0.562500\n",
      "max         1.000000\n",
      "Name: vote_average_normalized, dtype: float64\n",
      "----- popularity_standardized -----\n",
      "count    3252.000000\n",
      "mean        0.000000\n",
      "std         1.000000\n",
      "min        -0.384186\n",
      "25%        -0.223211\n",
      "50%        -0.152608\n",
      "75%        -0.003784\n",
      "max        46.371934\n",
      "Name: popularity_standardized, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('----- vote_average_normalized -----')\n",
    "print(df['vote_average_normalized'].describe())\n",
    "\n",
    "print('----- popularity_standardized -----')\n",
    "print(df['popularity_standardized'].describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to do what GPT-4 says, we'll see.\n",
    "\n",
    "> If you're concerned about the effect of these outliers on your subsequent \n",
    "> analysis, you might consider some additional preprocessing steps. You could, \n",
    "> for example, apply a logarithmic transformation to popularity before \n",
    "> standardizing, to reduce the impact of extreme values. Alternatively, you \n",
    "> might decide to remove movies that have a popularity above a certain \n",
    "> threshold, if you think these are likely to be anomalies or errors. The best \n",
    "> approach depends on your specific research question and analysis plan."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Correlation analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11661303487008848\n"
     ]
    }
   ],
   "source": [
    "correlation_coefficient = df['popularity_standardized'].corr(df['vote_average_normalized'])\n",
    "print(correlation_coefficient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
