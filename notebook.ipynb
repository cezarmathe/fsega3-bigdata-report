{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis on a TMDB datased of 15000 movies\n",
    "\n",
    "What is the relationship between a movie's popularity and its average vote? Do more popular movies receive higher average votes?\n",
    "\n",
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_CSV_FILE = 'datasets/tmdb-15000-movies.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_CSV_FILE, lineterminator='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "**1. Data cleaning**\n",
    "\n",
    "- remove non-English movies\n",
    "- remove movies with less than 100 votes\n",
    "- remove unused columns\n",
    "- remove duplicates \n",
    "- remove empty values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-English movies.\n",
    "df = df[df['original_language'] == 'en']\n",
    "# Remove movies with less than 100 votes.\n",
    "df = df[df['vote_count'] >= 100]\n",
    "# df = df[df['popularity'] <= 100]\n",
    "\n",
    "# Remove unused columns.\n",
    "df = df.drop(\n",
    "  [\n",
    "    'Unnamed: 0',\n",
    "    'adult',\n",
    "    'backdrop_path',\n",
    "    'cast',\n",
    "    'crew',\n",
    "    'genres',\n",
    "    'keywords',\n",
    "    'original_language',\n",
    "    'poster_path',\n",
    "    'release_date',\n",
    "    'video',\n",
    "    'vote_count',\n",
    "  ],\n",
    "  axis=1,\n",
    ")\n",
    "\n",
    "# Remove rows with null values.\n",
    "df = df.dropna()\n",
    "# Fill null values with empty string.\n",
    "df = df.fillna('')\n",
    "# Remove duplicate rows.\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# # Convert release_date to datetime.\n",
    "# df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Data cleaning**\n",
    "\n",
    "No specific transformation needed - `vote_average` and `popularity` are \n",
    "numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and now that you don't have to be perfect, you can be good\n",
      "but you're already perfect\n"
     ]
    }
   ],
   "source": [
    "print(\"and now that you don't have to be perfect, you can be good\")\n",
    "print(\"but you're already perfect\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Data standardization**\n",
    "\n",
    "We need to standardize `vote_average` and `popularity`.\n",
    "\n",
    "Let's find out their `min`, `max`, `mean`, `median` and `stddev`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- vote_average -----\n",
      "count    7637.000000\n",
      "mean        6.660168\n",
      "std         0.639222\n",
      "min         5.500000\n",
      "25%         6.200000\n",
      "50%         6.600000\n",
      "75%         7.100000\n",
      "max         8.700000\n",
      "Name: vote_average, dtype: float64\n",
      "----- popularity -----\n",
      "count    7637.000000\n",
      "mean       30.565745\n",
      "std       163.178378\n",
      "min         0.600000\n",
      "25%        11.447000\n",
      "50%        15.884000\n",
      "75%        25.577000\n",
      "max      9065.306000\n",
      "Name: popularity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('----- vote_average -----')\n",
    "print(df['vote_average'].describe())\n",
    "\n",
    "print('----- popularity -----')\n",
    "print(df['popularity'].describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vote_average` seems to fit in it's advertised range of __0__ - __10__, whereas\n",
    "`popularity` seems to have quite some outliers. We will do Min-Max Normalization\n",
    "for the former and Z-Score Standardization for the latter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vote_average_normalized'] = (df['vote_average'] - df['vote_average'].min()) / (df['vote_average'].max() - df['vote_average'].min())\n",
    "df['popularity_standardized'] = (df['popularity'] - df['popularity'].mean()) / df['popularity'].std()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "**1. Descriptive analysis**\n",
    "\n",
    "Let's re-run the descriptions on processed columns.\n",
    "\n",
    "I've run the two processes and here are the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- vote_average_normalized -----\n",
      "count    7637.000000\n",
      "mean        0.362552\n",
      "std         0.199757\n",
      "min         0.000000\n",
      "25%         0.218750\n",
      "50%         0.343750\n",
      "75%         0.500000\n",
      "max         1.000000\n",
      "Name: vote_average_normalized, dtype: float64\n",
      "----- popularity_standardized -----\n",
      "count    7637.000000\n",
      "mean        0.000000\n",
      "std         1.000000\n",
      "min        -0.183638\n",
      "25%        -0.117165\n",
      "50%        -0.089974\n",
      "75%        -0.030572\n",
      "max        55.367264\n",
      "Name: popularity_standardized, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('----- vote_average_normalized -----')\n",
    "print(df['vote_average_normalized'].describe())\n",
    "\n",
    "print('----- popularity_standardized -----')\n",
    "print(df['popularity_standardized'].describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to do what GPT-4 says, we'll see.\n",
    "\n",
    "> If you're concerned about the effect of these outliers on your subsequent \n",
    "> analysis, you might consider some additional preprocessing steps. You could, \n",
    "> for example, apply a logarithmic transformation to popularity before \n",
    "> standardizing, to reduce the impact of extreme values. Alternatively, you \n",
    "> might decide to remove movies that have a popularity above a certain \n",
    "> threshold, if you think these are likely to be anomalies or errors. The best \n",
    "> approach depends on your specific research question and analysis plan."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Correlation analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06969006927093538\n"
     ]
    }
   ],
   "source": [
    "correlation_coefficient = df['popularity_standardized'].corr(df['vote_average_normalized'])\n",
    "print(correlation_coefficient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
